{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0757a03-95b5-4067-a6e7-06e667affa51",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING INTERNSHIP @ CODE SOFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a2e27f-b281-4634-90cc-f738d89f47bd",
   "metadata": {},
   "source": [
    "## TASK 4 : Spam SMS Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed4d6f-ffbf-472d-a2b6-338abbdcd282",
   "metadata": {},
   "source": [
    "### The dataset is available at Kaggle : \n",
    "### https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2b9f4a-ad97-4845-8f5a-5cfcaa54ec92",
   "metadata": {},
   "source": [
    "### Context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e22c0-7ae5-4392-b970-4561a8c028d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Classifying emails into distinct labels can have a great impact on customer support. By using machine learning to label emails the system can set up queues containing emails of a specific category. This enables support personnel to handle request quicker and more easily by selecting a queue that match their expertise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9faedf9-7927-4586-9d42-8c75103bef95",
   "metadata": {},
   "source": [
    "### Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d3cd5d-b5d2-4b04-b6e7-2a8bffff82b4",
   "metadata": {},
   "source": [
    "- Build an AI model that can classify SMS messages as spam or legitimate. \n",
    "- Use techniques like TF-IDF or word embeddings with classifiers like Naive Bayes, Logistic Regression, or Support Vector Machines to identify spam messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d43985f-02e2-49f3-b363-a3d6776409c9",
   "metadata": {},
   "source": [
    "### We will follow the following steps for creating the model for spam sms detection\n",
    "Step 1: Data Collection\n",
    "Step 2: Data Preprocessing\n",
    "Step 3: Feature Engineering(TF-IDF)\n",
    "Step 4: Model Selection and Training(Logistic Regression)\n",
    "Step 5: Model Evaluation\n",
    "Step 6: Testing on random message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2bad44-177b-4954-b90c-7bae6b03ef3d",
   "metadata": {},
   "source": [
    "## Step 1:Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4352e196-ba78-4516-a9c6-6fb74e012f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17379603-225c-466b-b011-c6a8f5c924cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode bytes in position 606-607: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspam.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2050\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode bytes in position 606-607: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5ecd1-ee3e-435e-b01c-5580e8dfb8c4",
   "metadata": {},
   "source": [
    "Eerror occured as read_csv() function in pandas is 'utf-8', but sometimes CSV files contain characters that are not encoded in UTF-8. When pandas encounters such characters, it raises a UnicodeDecodeError because it cannot decode them using the specified encoding.\n",
    "we can use different encoding like ISO-8859-1 , etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbca70ae-2b12-449b-bed8-1f37fc0d1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe506298-07cf-4060-befd-586de8ee7daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1941cfeb-d8b3-4979-99c5-20499b72f61f",
   "metadata": {},
   "source": [
    "## Step 2:Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b5444f-652b-463e-ad86-6bb9086bd5ea",
   "metadata": {},
   "source": [
    "Our dataset has some extra columns (Unnamed: 2, Unnamed: 3, Unnamed: 4) that are empty (NaN).We will selcet only necessary columns i.e v1 and v2 thus cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b0fa8fd-4439-4575-97b9-77233f2ab0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df[['v1', 'v2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff529681-4710-4133-80dd-104131413c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b4531-a1c4-4d13-bb14-0de7e0ff4dfc",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e24b3e-8f99-47e1-af22-0bea0a9cecfb",
   "metadata": {},
   "source": [
    "We will be using the TF-IDF vectorizer from the scikit-learn library to convert the text data into numerical features.\n",
    "TF-IDF quantifies the importance of a word in a document relative to a collection of documents. It takes into account the frequency of a word in the document (Term Frequency) and the inverse document frequency (IDF), which measures how rare a word is across documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2692d323-85b2-45c5-a98f-7e6a911634bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82bd158e-8564-4ddb-8547-4d44a69797b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# Fit and transform text-data\n",
    "X = tfidf_vectorizer.fit_transform(df_cleaned['v2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d560478-d9b3-492c-8544-a92a853a8e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix: (5572, 8672)\n"
     ]
    }
   ],
   "source": [
    "# Display feature matrix shape\n",
    "print(\"Shape of feature matrix:\", X.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15b07837-6698-44a6-85fe-814f8e8d9f73",
   "metadata": {},
   "source": [
    "(5572, 8672) means we have 5572 samples(SMS messages) and 8672 features(unique words) after TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8225d6a-fb5e-4a7b-a099-20f1186177b0",
   "metadata": {},
   "source": [
    "## Step 4: Model Selection and Training\n",
    "Which means choosing a machine learning model (classifier) and training it on the feature matrix (X) along with the corresponding labels (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f818f6-f9d9-480b-9364-ea767d21f6fc",
   "metadata": {},
   "source": [
    "I chose Logistic Regression for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58cb08df-9832-4e69-a5f8-aba1def49cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing various requirements for logistic regression and its evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "746a1e07-8202-4708-b6a0-ae20c21fcd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_cleaned['v1'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fbc42e9-fcf0-43ed-ab34-f3442d581eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize \n",
    "model = LogisticRegression()\n",
    "# Training the model on the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e64136f-b0db-48d5-8e15-14632266db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on the test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d79f923c-151d-4fcf-8e3d-3858803d936c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9623318385650225\n"
     ]
    }
   ],
   "source": [
    "# Calculating its accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87787388-dcde-4480-b606-9b9d8a38d3ca",
   "metadata": {},
   "source": [
    "96.23% is considerably good so let's move on to next step.\n",
    "\n",
    "Step 5: Model Evaluation\n",
    "which is to evaluate the performance of the trained model using various metrics such as precision, recall, F1-score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cac1bf0-6c24-48cb-9452-f81409969507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing \n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ae59a37-dc85-41f9-b56a-02ea5e4de747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98       965\n",
      "        spam       1.00      0.72      0.84       150\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.98      0.86      0.91      1115\n",
      "weighted avg       0.96      0.96      0.96      1115\n",
      "\n",
      "Confusion Matrix:\n",
      "[[965   0]\n",
      " [ 42 108]]\n"
     ]
    }
   ],
   "source": [
    "# Printing classification report as well as confusion matrix\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0405b7-2fcd-4049-ae9c-71780677da93",
   "metadata": {},
   "source": [
    "Classification Report: \n",
    "Precision- It measures the accuracy of positive predictions. For the \"ham\" class the precision is 0.96, indicating 96% of the messages predicted as \"ham\" are indeed \"ham\". For the \"spam\" class, the precision is 1.00, meaning all messages predicted as \"spam\" are actually \"spam\".\n",
    "Recall(sensitivity)- It measures the proportion of actual positives that are correctly identified by the model. For the \"ham\" class, the recall is 1.00, indicating that all actual \"ham\" messages are correctly identified. For the \"spam\" class, the recall is 0.72, suggesting that 72% of the actual \"spam\" messages are correctly identified.\n",
    "F1-score: It is the harmonic mean of precision and recall. It provides a balance between precision and recall. For the \"ham\" class, the F1-score is 0.98, and for the \"spam\" class, the F1-score is 0.84.\n",
    "Support: Refers to the number of actual occurrences of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416a3e4e-1c6f-4df0-b30c-4e92d09c14e2",
   "metadata": {},
   "source": [
    "Confusion Matrix:\n",
    "The confusion matrix shows the counts of true positive TP, true negative TN, false positive FP, and false negative FN predictions.\n",
    "TP: 965 messages were correctly classified as \"ham\"\r",
    " T): 108 spam messages were correctly classified as \"spm\"\n",
    "FP: :  messages were incorrectly classified as \"pams\n",
    "(F): 42 spam messages were incorrectly classified as \"ham."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500f5767-d910-4720-b80f-f53c69965d05",
   "metadata": {},
   "source": [
    "## Step 6: testing on random message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "059f84f6-4308-40f1-a19a-24e130faf404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for the model to test on random essage\n",
    "def predict_spam_or_ham(message):\n",
    "    message_cleaned = [message]\n",
    "    # Transform using TF-IDF vectorization\n",
    "    message_vectorized = tfidf_vectorizer.transform(message_cleaned)\n",
    "    # Predict using model\n",
    "    prediction = model.predict(message_vectorized)\n",
    "    # Return the predicted class\n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68d392b9-e826-4698-9074-919cd1d31d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: ham\n"
     ]
    }
   ],
   "source": [
    "sample_message = \"Hey there! Just wanted to remind you about our meeting tomorrow at 10 AM. See you then!\"\n",
    "# Predicting using function\n",
    "predicted_class = predict_spam_or_ham(sample_message)\n",
    "# Printing the predicted class\n",
    "print(\"Predicted Class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c75f992c-973e-4e7e-a01f-16dd6e8f7219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: spam\n"
     ]
    }
   ],
   "source": [
    "sample_message2 =\"Congratulations! You've been selected as the winner of our special prize. Click the link to claim your reward now!\"\n",
    "predicted_class = predict_spam_or_ham(sample_message2)\n",
    "print(\"Predicted Class:\", predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
